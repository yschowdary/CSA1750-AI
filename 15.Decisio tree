import numpy as np

# Simple Node class for the decision tree
class Node:
    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):
        self.feature = feature  # Feature index to split on
        self.threshold = threshold  # Threshold value for splitting
        self.left = left  # Left subtree
        self.right = right  # Right subtree
        self.value = value  # Value if it is a leaf node

# Function to calculate Gini Impurity
def gini(y):
    proportions = np.bincount(y) / len(y)
    return 1 - np.sum(proportions ** 2)

# Function to split the dataset based on a feature and threshold
def split(X, y, feature, threshold):
    left_indices = np.where(X[:, feature] <= threshold)[0]
    right_indices = np.where(X[:, feature] > threshold)[0]
    return left_indices, right_indices

# Function to grow the decision tree
def build_tree(X, y, depth=0, max_depth=3):
    num_samples, num_features = X.shape
    num_labels = len(np.unique(y))

    # Stop if the stopping criteria is met
    if depth >= max_depth or num_labels == 1:
        leaf_value = np.argmax(np.bincount(y))
        return Node(value=leaf_value)

    best_feature, best_threshold = None, None
    best_gini = 1
    best_splits = None

    # Iterate over features and thresholds to find the best split
    for feature in range(num_features):
        thresholds = np.unique(X[:, feature])
        for threshold in thresholds:
            left_indices, right_indices = split(X, y, feature, threshold)
            if len(left_indices) == 0 or len(right_indices) == 0:
                continue

            # Calculate weighted Gini impurity for the split
            left_gini = gini(y[left_indices])
            right_gini = gini(y[right_indices])
            weighted_gini = (len(left_indices) * left_gini + len(right_indices) * right_gini) / num_samples

            if weighted_gini < best_gini:
                best_gini = weighted_gini
                best_feature = feature
                best_threshold = threshold
                best_splits = (left_indices, right_indices)

    if best_splits is None:
        leaf_value = np.argmax(np.bincount(y))
        return Node(value=leaf_value)

    left_indices, right_indices = best_splits
    left_subtree = build_tree(X[left_indices], y[left_indices], depth + 1, max_depth)
    right_subtree = build_tree(X[right_indices], y[right_indices], depth + 1, max_depth)
    return Node(feature=best_feature, threshold=best_threshold, left=left_subtree, right=right_subtree)

# Function to make predictions using the decision tree
def predict(tree, X):
    if tree.value is not None:
        return tree.value
    if X[tree.feature] <= tree.threshold:
        return predict(tree.left, X)
    else:
        return predict(tree.right, X)

# Train the decision tree on a simple dataset
if __name__ == "__main__":
    # Simple dataset (X: features, y: labels)
    X = np.array([[2.771244718, 1.784783929],
                  [1.728571309, 1.169761413],
                  [3.678319846, 2.812813571],
                  [3.961043357, 2.61995032],
                  [2.999208922, 2.209014212],
                  [7.497545867, 3.162953546],
                  [9.00220326,  3.339047188],
                  [7.444542326, 0.476683375],
                  [10.12493903, 3.234550982],
                  [6.642287351, 3.319983761]])

    y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])

    # Build the decision tree
    tree = build_tree(X, y, max_depth=3)

    # Predict a sample
    sample = np.array([6.0, 3.0])
    prediction = predict(tree, sample)

    print(f"Predicted class for sample {sample}: {prediction}")

